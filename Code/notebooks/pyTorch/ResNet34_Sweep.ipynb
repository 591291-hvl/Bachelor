{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bruker ResNet arkitektur kombinert med PyTorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "\n",
    "#other libraries\n",
    "# from tqdm import tqdm\n",
    "# import time\n",
    "# import random\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import h5py\n",
    "\n",
    "#torch specific\n",
    "import torch\n",
    "import torchvision as torchv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch import Tensor\n",
    "from torch.utils import data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wandb\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\student\\Desktop\\Daniel og Sayna\\Bachelor\\Code\\src\\trainers.py:168: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if config.optimizer is 'adam':\n"
     ]
    }
   ],
   "source": [
    "module_path = str(Path.cwd().parents[0].parents[0] / \"src\")\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from dataloader import *\n",
    "# from plotCreator import *\n",
    "import trainers\n",
    "\n",
    "\n",
    "data_path0 = str(Path.cwd().parents[0].parents[0] / \"data\" / \"bh\" / \"BH_n4_M10_res50_15000_events.h5\")\n",
    "data_path1 = str(Path.cwd().parents[0].parents[0] / \"data\" / \"sph\" / \"PP13-Sphaleron-THR9-FRZ15-NB0-NSUBPALL_res50_15000_events.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bhArray = dataToArray(data_path0)\n",
    "sphArray = dataToArray(data_path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 50, 50, 3)\n",
      "(15000, 50, 50, 3)\n"
     ]
    }
   ],
   "source": [
    "print(bhArray.shape)\n",
    "print(sphArray.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataArray = np.concatenate((bhArray,sphArray),axis=0)\n",
    "labelsArray = np.concatenate((np.zeros(np.shape(bhArray)[0]),np.ones(np.shape(sphArray)[0])),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData, testData, trainLabels, testLabels = train_test_split(dataArray, labelsArray, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = torch.from_numpy(trainData)\n",
    "testData = torch.from_numpy(testData)\n",
    "trainLabels = torch.from_numpy(trainLabels)\n",
    "testLabels = torch.from_numpy(testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.utils.data.TensorDataset(trainData, trainLabels)\n",
    "test = torch.utils.data.TensorDataset(testData, testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ResidualBlock block to be used in ResNet model\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride = 1, downsample = None):\n",
    "\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "                        nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = stride, padding = 1),\n",
    "                        nn.BatchNorm2d(out_channels),\n",
    "                        nn.ReLU())\n",
    "        self.conv2 = nn.Sequential(\n",
    "                        nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1),\n",
    "                        nn.BatchNorm2d(out_channels))\n",
    "        self.downsample = downsample\n",
    "        self.relu = nn.ReLU()\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetModel(nn.Module):\n",
    "    def __init__(self, dropout, block, layers, num_classes=2):\n",
    "\n",
    "        super(ResNetModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=2, padding=3)\n",
    "        self.batch1 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.layer0 = self._make_layer(block, 64, layers[0], stride=1)\n",
    "        self.layer1 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer2 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        # self.avgPool = nn.AvgPool2d(7, stride=1)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.fc0 = nn.Linear(2048, num_classes)\n",
    "\n",
    "    def forward(self, x:Tensor):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x,2)\n",
    "\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        # x = self.avgPool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc0(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    #https://blog.paperspace.com/writing-resnet-from-scratch-in-pytorch/\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes:\n",
    "            \n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(planes),\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 25, 25]           9,472\n",
      "       BatchNorm2d-2           [-1, 64, 25, 25]             128\n",
      "            Conv2d-3           [-1, 64, 12, 12]          36,928\n",
      "       BatchNorm2d-4           [-1, 64, 12, 12]             128\n",
      "              ReLU-5           [-1, 64, 12, 12]               0\n",
      "            Conv2d-6           [-1, 64, 12, 12]          36,928\n",
      "       BatchNorm2d-7           [-1, 64, 12, 12]             128\n",
      "              ReLU-8           [-1, 64, 12, 12]               0\n",
      "     ResidualBlock-9           [-1, 64, 12, 12]               0\n",
      "           Conv2d-10           [-1, 64, 12, 12]          36,928\n",
      "      BatchNorm2d-11           [-1, 64, 12, 12]             128\n",
      "             ReLU-12           [-1, 64, 12, 12]               0\n",
      "           Conv2d-13           [-1, 64, 12, 12]          36,928\n",
      "      BatchNorm2d-14           [-1, 64, 12, 12]             128\n",
      "             ReLU-15           [-1, 64, 12, 12]               0\n",
      "    ResidualBlock-16           [-1, 64, 12, 12]               0\n",
      "           Conv2d-17           [-1, 64, 12, 12]          36,928\n",
      "      BatchNorm2d-18           [-1, 64, 12, 12]             128\n",
      "             ReLU-19           [-1, 64, 12, 12]               0\n",
      "           Conv2d-20           [-1, 64, 12, 12]          36,928\n",
      "      BatchNorm2d-21           [-1, 64, 12, 12]             128\n",
      "             ReLU-22           [-1, 64, 12, 12]               0\n",
      "    ResidualBlock-23           [-1, 64, 12, 12]               0\n",
      "           Conv2d-24            [-1, 128, 6, 6]          73,856\n",
      "      BatchNorm2d-25            [-1, 128, 6, 6]             256\n",
      "             ReLU-26            [-1, 128, 6, 6]               0\n",
      "           Conv2d-27            [-1, 128, 6, 6]         147,584\n",
      "      BatchNorm2d-28            [-1, 128, 6, 6]             256\n",
      "           Conv2d-29            [-1, 128, 6, 6]           8,320\n",
      "      BatchNorm2d-30            [-1, 128, 6, 6]             256\n",
      "             ReLU-31            [-1, 128, 6, 6]               0\n",
      "    ResidualBlock-32            [-1, 128, 6, 6]               0\n",
      "           Conv2d-33            [-1, 128, 6, 6]         147,584\n",
      "      BatchNorm2d-34            [-1, 128, 6, 6]             256\n",
      "             ReLU-35            [-1, 128, 6, 6]               0\n",
      "           Conv2d-36            [-1, 128, 6, 6]         147,584\n",
      "      BatchNorm2d-37            [-1, 128, 6, 6]             256\n",
      "             ReLU-38            [-1, 128, 6, 6]               0\n",
      "    ResidualBlock-39            [-1, 128, 6, 6]               0\n",
      "           Conv2d-40            [-1, 128, 6, 6]         147,584\n",
      "      BatchNorm2d-41            [-1, 128, 6, 6]             256\n",
      "             ReLU-42            [-1, 128, 6, 6]               0\n",
      "           Conv2d-43            [-1, 128, 6, 6]         147,584\n",
      "      BatchNorm2d-44            [-1, 128, 6, 6]             256\n",
      "             ReLU-45            [-1, 128, 6, 6]               0\n",
      "    ResidualBlock-46            [-1, 128, 6, 6]               0\n",
      "           Conv2d-47            [-1, 128, 6, 6]         147,584\n",
      "      BatchNorm2d-48            [-1, 128, 6, 6]             256\n",
      "             ReLU-49            [-1, 128, 6, 6]               0\n",
      "           Conv2d-50            [-1, 128, 6, 6]         147,584\n",
      "      BatchNorm2d-51            [-1, 128, 6, 6]             256\n",
      "             ReLU-52            [-1, 128, 6, 6]               0\n",
      "    ResidualBlock-53            [-1, 128, 6, 6]               0\n",
      "           Conv2d-54            [-1, 256, 3, 3]         295,168\n",
      "      BatchNorm2d-55            [-1, 256, 3, 3]             512\n",
      "             ReLU-56            [-1, 256, 3, 3]               0\n",
      "           Conv2d-57            [-1, 256, 3, 3]         590,080\n",
      "      BatchNorm2d-58            [-1, 256, 3, 3]             512\n",
      "           Conv2d-59            [-1, 256, 3, 3]          33,024\n",
      "      BatchNorm2d-60            [-1, 256, 3, 3]             512\n",
      "             ReLU-61            [-1, 256, 3, 3]               0\n",
      "    ResidualBlock-62            [-1, 256, 3, 3]               0\n",
      "           Conv2d-63            [-1, 256, 3, 3]         590,080\n",
      "      BatchNorm2d-64            [-1, 256, 3, 3]             512\n",
      "             ReLU-65            [-1, 256, 3, 3]               0\n",
      "           Conv2d-66            [-1, 256, 3, 3]         590,080\n",
      "      BatchNorm2d-67            [-1, 256, 3, 3]             512\n",
      "             ReLU-68            [-1, 256, 3, 3]               0\n",
      "    ResidualBlock-69            [-1, 256, 3, 3]               0\n",
      "           Conv2d-70            [-1, 256, 3, 3]         590,080\n",
      "      BatchNorm2d-71            [-1, 256, 3, 3]             512\n",
      "             ReLU-72            [-1, 256, 3, 3]               0\n",
      "           Conv2d-73            [-1, 256, 3, 3]         590,080\n",
      "      BatchNorm2d-74            [-1, 256, 3, 3]             512\n",
      "             ReLU-75            [-1, 256, 3, 3]               0\n",
      "    ResidualBlock-76            [-1, 256, 3, 3]               0\n",
      "           Conv2d-77            [-1, 256, 3, 3]         590,080\n",
      "      BatchNorm2d-78            [-1, 256, 3, 3]             512\n",
      "             ReLU-79            [-1, 256, 3, 3]               0\n",
      "           Conv2d-80            [-1, 256, 3, 3]         590,080\n",
      "      BatchNorm2d-81            [-1, 256, 3, 3]             512\n",
      "             ReLU-82            [-1, 256, 3, 3]               0\n",
      "    ResidualBlock-83            [-1, 256, 3, 3]               0\n",
      "           Conv2d-84            [-1, 256, 3, 3]         590,080\n",
      "      BatchNorm2d-85            [-1, 256, 3, 3]             512\n",
      "             ReLU-86            [-1, 256, 3, 3]               0\n",
      "           Conv2d-87            [-1, 256, 3, 3]         590,080\n",
      "      BatchNorm2d-88            [-1, 256, 3, 3]             512\n",
      "             ReLU-89            [-1, 256, 3, 3]               0\n",
      "    ResidualBlock-90            [-1, 256, 3, 3]               0\n",
      "           Conv2d-91            [-1, 256, 3, 3]         590,080\n",
      "      BatchNorm2d-92            [-1, 256, 3, 3]             512\n",
      "             ReLU-93            [-1, 256, 3, 3]               0\n",
      "           Conv2d-94            [-1, 256, 3, 3]         590,080\n",
      "      BatchNorm2d-95            [-1, 256, 3, 3]             512\n",
      "             ReLU-96            [-1, 256, 3, 3]               0\n",
      "    ResidualBlock-97            [-1, 256, 3, 3]               0\n",
      "           Conv2d-98            [-1, 512, 2, 2]       1,180,160\n",
      "      BatchNorm2d-99            [-1, 512, 2, 2]           1,024\n",
      "            ReLU-100            [-1, 512, 2, 2]               0\n",
      "          Conv2d-101            [-1, 512, 2, 2]       2,359,808\n",
      "     BatchNorm2d-102            [-1, 512, 2, 2]           1,024\n",
      "          Conv2d-103            [-1, 512, 2, 2]         131,584\n",
      "     BatchNorm2d-104            [-1, 512, 2, 2]           1,024\n",
      "            ReLU-105            [-1, 512, 2, 2]               0\n",
      "   ResidualBlock-106            [-1, 512, 2, 2]               0\n",
      "          Conv2d-107            [-1, 512, 2, 2]       2,359,808\n",
      "     BatchNorm2d-108            [-1, 512, 2, 2]           1,024\n",
      "            ReLU-109            [-1, 512, 2, 2]               0\n",
      "          Conv2d-110            [-1, 512, 2, 2]       2,359,808\n",
      "     BatchNorm2d-111            [-1, 512, 2, 2]           1,024\n",
      "            ReLU-112            [-1, 512, 2, 2]               0\n",
      "   ResidualBlock-113            [-1, 512, 2, 2]               0\n",
      "          Conv2d-114            [-1, 512, 2, 2]       2,359,808\n",
      "     BatchNorm2d-115            [-1, 512, 2, 2]           1,024\n",
      "            ReLU-116            [-1, 512, 2, 2]               0\n",
      "          Conv2d-117            [-1, 512, 2, 2]       2,359,808\n",
      "     BatchNorm2d-118            [-1, 512, 2, 2]           1,024\n",
      "            ReLU-119            [-1, 512, 2, 2]               0\n",
      "   ResidualBlock-120            [-1, 512, 2, 2]               0\n",
      "          Linear-121                    [-1, 2]           4,098\n",
      "================================================================\n",
      "Total params: 21,297,282\n",
      "Trainable params: 21,297,282\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.03\n",
      "Forward/backward pass size (MB): 4.27\n",
      "Params size (MB): 81.24\n",
      "Estimated Total Size (MB): 85.55\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "model = ResNetModel(0,ResidualBlock, [3, 4, 6, 3]).to(device)\n",
    "summary(model, (3, 50, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: wpruy9hi\n",
      "Sweep URL: https://wandb.ai/g13hvl2023/ResNet34_Sweep/sweeps/wpruy9hi\n"
     ]
    }
   ],
   "source": [
    "sweep_configuration = {\n",
    "    'method': 'grid',\n",
    "    'name': 'sweep',\n",
    "    'metric': {\n",
    "        'goal': 'maximize', \n",
    "        'name': 'Test accuracy'\n",
    "        },\n",
    "    'parameters': {\n",
    "        'loss': {'values': ['customLoss', 'hinge', 'cross']},\n",
    "        'epoch': {'values': [20]},\n",
    "        'batch_size': {'values': [50]},\n",
    "        'gamma': {'values': [1]},\n",
    "        'learning_rate': {'values': [0.01]},\n",
    "        'optimizer': {'values': ['adam']},\n",
    "        'dropout': {'values': [0]}\n",
    "     }\n",
    "}\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=\"ResNet34_Sweep\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wrapper\n",
    "def trainFunction():\n",
    "    with wandb.init(project=\"ResNet34_Sweep\", name=\"ResNet34_Sweep\"):\n",
    "        config = wandb.config\n",
    "        model = ResNetModel(config.dropout, ResidualBlock, [3, 4, 6, 3]).to(device)\n",
    "        trainers.sweep(model, train, test, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: kqofyxtm\n",
      "Sweep URL: https://wandb.ai/g13hvl2023/ResNet34_Sweep/sweeps/kqofyxtm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: lliztns9 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: customLoss\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m591291\u001b[0m (\u001b[33mg13hvl2023\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\student\\Desktop\\Daniel og Sayna\\Bachelor\\Code\\notebooks\\pyTorch\\wandb\\run-20230324_120847-lliztns9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/g13hvl2023/ResNet34_Sweep/runs/lliztns9' target=\"_blank\">ResNet34_Sweep</a></strong> to <a href='https://wandb.ai/g13hvl2023/ResNet34_Sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/g13hvl2023/ResNet34_Sweep/sweeps/kqofyxtm' target=\"_blank\">https://wandb.ai/g13hvl2023/ResNet34_Sweep/sweeps/kqofyxtm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/g13hvl2023/ResNet34_Sweep' target=\"_blank\">https://wandb.ai/g13hvl2023/ResNet34_Sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/g13hvl2023/ResNet34_Sweep/sweeps/kqofyxtm' target=\"_blank\">https://wandb.ai/g13hvl2023/ResNet34_Sweep/sweeps/kqofyxtm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/g13hvl2023/ResNet34_Sweep/runs/lliztns9' target=\"_blank\">https://wandb.ai/g13hvl2023/ResNet34_Sweep/runs/lliztns9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e745f4e4ff747a1afcbaf63aa8c9f0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.541 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.002376…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>BH accuracy</td><td>▁▆▇▇▇▇▇▇▇▇█▆▇█▇▇▇▇▇█▇</td></tr><tr><td>SPH accuracy</td><td>▁▆▇▇▆▇█▇██▅▄█▇▆█▇▇▆▂█</td></tr><tr><td>Test accuracy</td><td>▁▇▇████████▆███████▇█</td></tr><tr><td>Test epoch_loss</td><td>█▃▂▂▁▁▁▁▁▁▁▃▁▁▁▁▁▂▁▂▁</td></tr><tr><td>Test precision</td><td>▁▆▇▇▆▇█▇██▅▄█▇▆█▇▇▆▂█</td></tr><tr><td>Test recall</td><td>▁▆▆▇▇▇▇▇▇▇█▆▇▇▇▇▇▇▇█▇</td></tr><tr><td>Train accuracy</td><td>▁▄▄▃▆▆▇▇▇▇▆▅▇▇▇███▆▇</td></tr><tr><td>Train epoch_loss</td><td>█▅▅▅▃▃▃▂▂▂▃▄▂▂▂▁▁▁▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>BH accuracy</td><td>0.86151</td></tr><tr><td>SPH accuracy</td><td>0.96279</td></tr><tr><td>Test accuracy</td><td>0.9116</td></tr><tr><td>Test epoch_loss</td><td>0.23482</td></tr><tr><td>Test precision</td><td>0.96279</td></tr><tr><td>Test recall</td><td>0.87183</td></tr><tr><td>Train accuracy</td><td>0.93027</td></tr><tr><td>Train epoch_loss</td><td>0.17932</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ResNet34_Sweep</strong> at: <a href='https://wandb.ai/g13hvl2023/ResNet34_Sweep/runs/lliztns9' target=\"_blank\">https://wandb.ai/g13hvl2023/ResNet34_Sweep/runs/lliztns9</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230324_120847-lliztns9\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: un9ll4t9 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: hinge\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\student\\Desktop\\Daniel og Sayna\\Bachelor\\Code\\notebooks\\pyTorch\\wandb\\run-20230324_122412-un9ll4t9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/g13hvl2023/ResNet34_Sweep/runs/un9ll4t9' target=\"_blank\">ResNet34_Sweep</a></strong> to <a href='https://wandb.ai/g13hvl2023/ResNet34_Sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/g13hvl2023/ResNet34_Sweep/sweeps/kqofyxtm' target=\"_blank\">https://wandb.ai/g13hvl2023/ResNet34_Sweep/sweeps/kqofyxtm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/g13hvl2023/ResNet34_Sweep' target=\"_blank\">https://wandb.ai/g13hvl2023/ResNet34_Sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/g13hvl2023/ResNet34_Sweep/sweeps/kqofyxtm' target=\"_blank\">https://wandb.ai/g13hvl2023/ResNet34_Sweep/sweeps/kqofyxtm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/g13hvl2023/ResNet34_Sweep/runs/un9ll4t9' target=\"_blank\">https://wandb.ai/g13hvl2023/ResNet34_Sweep/runs/un9ll4t9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\student\\Anaconda3\\envs\\DAT191-gpu\\lib\\site-packages\\torch\\autograd\\__init__.py:197: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [50, 2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:24.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "c:\\Users\\student\\Anaconda3\\envs\\DAT191-gpu\\lib\\site-packages\\torch\\autograd\\__init__.py:197: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [50, 2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:24.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "c:\\Users\\student\\Anaconda3\\envs\\DAT191-gpu\\lib\\site-packages\\torch\\autograd\\__init__.py:197: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [50, 2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:24.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "c:\\Users\\student\\Anaconda3\\envs\\DAT191-gpu\\lib\\site-packages\\torch\\autograd\\__init__.py:197: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [50, 2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:24.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "c:\\Users\\student\\Anaconda3\\envs\\DAT191-gpu\\lib\\site-packages\\torch\\autograd\\__init__.py:197: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [50, 2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:24.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "c:\\Users\\student\\Anaconda3\\envs\\DAT191-gpu\\lib\\site-packages\\torch\\autograd\\__init__.py:197: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [50, 2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:24.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "c:\\Users\\student\\Anaconda3\\envs\\DAT191-gpu\\lib\\site-packages\\torch\\autograd\\__init__.py:197: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [50, 2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:24.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "c:\\Users\\student\\Anaconda3\\envs\\DAT191-gpu\\lib\\site-packages\\torch\\autograd\\__init__.py:197: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [50, 2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:24.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "c:\\Users\\student\\Anaconda3\\envs\\DAT191-gpu\\lib\\site-packages\\torch\\autograd\\__init__.py:197: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [50, 2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:24.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "c:\\Users\\student\\Anaconda3\\envs\\DAT191-gpu\\lib\\site-packages\\torch\\autograd\\__init__.py:197: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [50, 2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:24.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "c:\\Users\\student\\Anaconda3\\envs\\DAT191-gpu\\lib\\site-packages\\torch\\autograd\\__init__.py:197: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [50, 2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:24.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "c:\\Users\\student\\Anaconda3\\envs\\DAT191-gpu\\lib\\site-packages\\torch\\autograd\\__init__.py:197: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [50, 2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:24.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "c:\\Users\\student\\Anaconda3\\envs\\DAT191-gpu\\lib\\site-packages\\torch\\autograd\\__init__.py:197: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [50, 2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:24.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "c:\\Users\\student\\Anaconda3\\envs\\DAT191-gpu\\lib\\site-packages\\torch\\autograd\\__init__.py:197: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [50, 2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:24.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "c:\\Users\\student\\Anaconda3\\envs\\DAT191-gpu\\lib\\site-packages\\torch\\autograd\\__init__.py:197: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [50, 2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:24.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "c:\\Users\\student\\Anaconda3\\envs\\DAT191-gpu\\lib\\site-packages\\torch\\autograd\\__init__.py:197: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [50, 2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:24.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "c:\\Users\\student\\Anaconda3\\envs\\DAT191-gpu\\lib\\site-packages\\torch\\autograd\\__init__.py:197: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [50, 2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:24.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "c:\\Users\\student\\Anaconda3\\envs\\DAT191-gpu\\lib\\site-packages\\torch\\autograd\\__init__.py:197: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [50, 2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:24.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "c:\\Users\\student\\Anaconda3\\envs\\DAT191-gpu\\lib\\site-packages\\torch\\autograd\\__init__.py:197: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [50, 2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:24.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "c:\\Users\\student\\Anaconda3\\envs\\DAT191-gpu\\lib\\site-packages\\torch\\autograd\\__init__.py:197: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [50, 2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\Resize.cpp:24.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e36c25684ba40b8b15b26716b967d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>BH accuracy</td><td>▁▇▅▅▅█▆▆▄▅▅▃▅▆▆▆▇▅▆▅▅</td></tr><tr><td>SPH accuracy</td><td>▁▆███▆███████▇▇█▇████</td></tr><tr><td>Test accuracy</td><td>▁▇███▇███████████████</td></tr><tr><td>Test epoch_loss</td><td>█▃▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Test precision</td><td>▁▆███▆███████▇▇█▇████</td></tr><tr><td>Test recall</td><td>▁▇▇▇▇█▇▇▇▇▇▇▇▇▇▇█▇▇▇▇</td></tr><tr><td>Train accuracy</td><td>▁▄▅▆▆▆▇▇▇▇▇▇▇▇▇█████</td></tr><tr><td>Train epoch_loss</td><td>█▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>BH accuracy</td><td>0.86231</td></tr><tr><td>SPH accuracy</td><td>0.94662</td></tr><tr><td>Test accuracy</td><td>0.904</td></tr><tr><td>Test epoch_loss</td><td>0.09979</td></tr><tr><td>Test precision</td><td>0.94662</td></tr><tr><td>Test recall</td><td>0.87057</td></tr><tr><td>Train accuracy</td><td>0.96773</td></tr><tr><td>Train epoch_loss</td><td>0.03413</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ResNet34_Sweep</strong> at: <a href='https://wandb.ai/g13hvl2023/ResNet34_Sweep/runs/un9ll4t9' target=\"_blank\">https://wandb.ai/g13hvl2023/ResNet34_Sweep/runs/un9ll4t9</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230324_122412-un9ll4t9\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wbbobjp2 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 50\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepoch: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\student\\Desktop\\Daniel og Sayna\\Bachelor\\Code\\notebooks\\pyTorch\\wandb\\run-20230324_123929-wbbobjp2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/g13hvl2023/ResNet34_Sweep/runs/wbbobjp2' target=\"_blank\">ResNet34_Sweep</a></strong> to <a href='https://wandb.ai/g13hvl2023/ResNet34_Sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/g13hvl2023/ResNet34_Sweep/sweeps/kqofyxtm' target=\"_blank\">https://wandb.ai/g13hvl2023/ResNet34_Sweep/sweeps/kqofyxtm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/g13hvl2023/ResNet34_Sweep' target=\"_blank\">https://wandb.ai/g13hvl2023/ResNet34_Sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/g13hvl2023/ResNet34_Sweep/sweeps/kqofyxtm' target=\"_blank\">https://wandb.ai/g13hvl2023/ResNet34_Sweep/sweeps/kqofyxtm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/g13hvl2023/ResNet34_Sweep/runs/wbbobjp2' target=\"_blank\">https://wandb.ai/g13hvl2023/ResNet34_Sweep/runs/wbbobjp2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c4e560ccf7b4a8e8e83b96358c3c145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.009 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.144232…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>BH accuracy</td><td>▁▇▄█▃▃▅█▃▇▆▆▆▆▆▅▇▆▆▆▆</td></tr><tr><td>SPH accuracy</td><td>▁▇█▇███▆█▇▇▇▇███▇████</td></tr><tr><td>Test accuracy</td><td>▁▇█████▇█████████████</td></tr><tr><td>Test epoch_loss</td><td>█▂▁▁▂▂▂▄▄▃▃▃▄▄▄▃▄▄▄▅▆</td></tr><tr><td>Test precision</td><td>▁▇█▇███▆█▇▇▇▇███▇████</td></tr><tr><td>Test recall</td><td>▁█▇█▆▆▇█▆█▇▇▇▇▇▇█▇▇▇▇</td></tr><tr><td>Train accuracy</td><td>▁▄▅▅▆▆▇▇▇▇▇▇████████</td></tr><tr><td>Train epoch_loss</td><td>█▅▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>BH accuracy</td><td>0.88367</td></tr><tr><td>SPH accuracy</td><td>0.91992</td></tr><tr><td>Test accuracy</td><td>0.9016</td></tr><tr><td>Test epoch_loss</td><td>0.56401</td></tr><tr><td>Test precision</td><td>0.91992</td></tr><tr><td>Test recall</td><td>0.88554</td></tr><tr><td>Train accuracy</td><td>0.9948</td></tr><tr><td>Train epoch_loss</td><td>0.01553</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ResNet34_Sweep</strong> at: <a href='https://wandb.ai/g13hvl2023/ResNet34_Sweep/runs/wbbobjp2' target=\"_blank\">https://wandb.ai/g13hvl2023/ResNet34_Sweep/runs/wbbobjp2</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230324_123929-wbbobjp2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Exiting.\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=\"ResNet34_Sweep\")\n",
    "wandb.agent(sweep_id, function=trainFunction, count=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e9fb61b4c72117f5d7b2c8d6242145ec5a71ca8cd00b47ff0a0694a5bc1f5f2f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
