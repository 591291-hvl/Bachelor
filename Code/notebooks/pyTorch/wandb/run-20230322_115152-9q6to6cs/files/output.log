output:  [-3.5958982 -6.2593994] labels:  0.0
output:  [-2.1240792 -1.8365464] labels:  1.0
output:  [-2.16771  -3.419354] labels:  1.0
output:  [-2.4755707 -4.794572 ] labels:  0.0
output:  [-2.0339475 -3.8222013] labels:  1.0
output:  [-2.1021898 -2.3488817] labels:  1.0
output:  [-4.635528 -8.997938] labels:  0.0
output:  [-1.9126318 -0.7855661] labels:  1.0
output:  [-1.8598797 -1.4024425] labels:  0.0
output:  [-3.6385627 -6.546128 ] labels:  0.0
output:  1 labels:  1.0
output:  1 labels:  1.0
output:  1 labels:  1.0
output:  0 labels:  0.0
output:  0 labels:  0.0
output:  1 labels:  0.0
output:  0 labels:  1.0
output:  1 labels:  1.0
output:  0 labels:  0.0
output:  0 labels:  0.0
tensor(0.2493, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3739, device='cuda:0')
tensor(0.2800, requires_grad=True)
c:\Users\student\Desktop\Daniel og Sayna\Bachelor\Code\src\lossFunctions.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  input = torch.tensor(input)
tensor(0.2493, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3739, device='cuda:0')
tensor(0.2800, requires_grad=True)
tensor(0.2493, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3739, device='cuda:0')
tensor(0.2800, requires_grad=True)
tensor(0.2493, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3739, device='cuda:0')
tensor(0.2800, requires_grad=True)
tensor(0.2493, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3739, device='cuda:0')
tensor(0.2800, requires_grad=True)
tensor(0.2493, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3739, device='cuda:0')
tensor(0.2800, requires_grad=True)
tensor(0.2493, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3739, device='cuda:0')
tensor(0.2800, requires_grad=True)
tensor(0.2493, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3739, device='cuda:0')
tensor(0.2800, requires_grad=True)
tensor(0.2493, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3739, device='cuda:0')
tensor(0.2800, requires_grad=True)
tensor(0.2493, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3739, device='cuda:0')
tensor(0.2800, requires_grad=True)
tensor(0.2493, device='cuda:0', grad_fn=<NllLossBackward0>)
tensor(0.3739, device='cuda:0')
tensor(0.2800, requires_grad=True)
(15000, 50, 50, 3)
(15000, 50, 50, 3)
Running on the GPU
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 16, 48, 48]             448
            Conv2d-2           [-1, 64, 22, 22]           9,280
            Conv2d-3            [-1, 256, 9, 9]         147,712
            Linear-4                  [-1, 128]         295,040
           Dropout-5                  [-1, 128]               0
            Linear-6                    [-1, 2]             258
================================================================
Total params: 452,738
Trainable params: 452,738
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.03
Forward/backward pass size (MB): 0.68
Params size (MB): 1.73
Estimated Total Size (MB): 2.43
----------------------------------------------------------------